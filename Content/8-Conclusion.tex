%% ==============================
\chapter{\iflanguage{ngerman}{Zusammensaffung und Ausblick}{Conclusion and Outlook}}
\label{sec:conclusion_and_outlook}
%% ==============================
Influence of high network utilization

In  this thesis, a concept for distributed control of multiple robots with the \gls{r2c} framework was proposed, implemented and evaluated. The main objective was to design an alternative concept to the shared-memory design of \gls{r2c}, that enables distributed control of robot systems with the framework. Further, the concept should allow for different distributed control architectures, while minimizing the impact of change on existing controllers and drivers. \newline
The proposed architecture consists of a central controller manager and multiple sub-controller managers. New distributed \glspl{si} and \glspl{ci} have been introduced, as well as wrapper classes for the old \glspl{si} and \glspl{ci}. The sub-controller managers can then export their \gls{si} and \gls{ci}, allowing the exchange of hardware states and commands over the network. This concept relies heavily on existing concepts from \gls{ros2} such as topics, publisher, subscribers and services.\newline
With the proposed concept, two different distributed control architectures can be realized, namely "distributed drivers" and "distributed chained control". In the "distributed drivers" case, the controller runs on one machine, while the drivers handling communication with the hardware run on separate machines. This allows for example to flexibly control multiple robots, where the robots are controller on one computer by a central controller and the communication is handled separately by the sub-controller managers on different machines. With the "distributed chained control" case, controllers can be distributed on multiple machines and then chained to each other. Meaning, the output of a controller running on the central controller manager can be chained to the input of a controller running in a sub-controller manager on a different machine. This allows for hierarchical control architectures, where a high level controller runs on the central controller manager, while low level controller and communication is handled in the sub-controller managers on different machines.\newline
The concept was evaluated using three different off the shelf laptops and two KUKA KR3 R540. The influence of the used \gls{rmw} and the choosen \gls{qos} on the performance was investigated. For the evaluation, the two robots were coupled with a distance sensor. Then trajectories were executed, where the distance between the two end effectors of the robot should stay the same if the robots move simultaneous. During the execution of the motion, the distance change has been plotted.\newline
In the first experiment, the "distributed drivers" case was investigated. A central controller manager with a \textit{JointTrajectoryController} was used. Additionally, two sub-controller managers, with each of them having the drivers for communication with the robots running, were used. The \glspl{rmw} Fast DDS, Cyclone DDS and Connext DDS were used. A reliable and a sensor profile were tested as well. The commands issued for the execution of the trajectories were all created by the central controller manager's \textit{JointTrajectoryController}. The measurements indicated that due to the implicit time synchronization, the robots move in a nearly synchronous manner. Small deviation in the micrometer range could be measured. The movement was repeatable and reliable. Further, the sensor profile always performed better than the reliable profile, regardless of the \gls{rmw}. The results for Fast DDS and Cyclone DDS are nearly similar. Both outperform Connext DDS. With both Fast DDS and Cyclone DDS no jitter of the robots could be observed. However, with Connext DDS as \gls{rmw} there was a permanent jitter during the execution present.\newline
In the second experiment, the "distributed chained control" case was investigated. In this experiment, each of the sub-controller managers had in addition to the drivers a \textit{JointTrajectoryController} running. The input of the \textit{JointTrajectoryControllers} were chained to the output of the central controller manager's \textit{ForwardCommandController}. The \textit{ForwardCommandController} did then send the same  "Trajectory message" via the \glspl{ri} to the \textit{JointTrajectoryControllers}. The measurements for this result do indicate that the concept works. No jitter could be measured. However, as the commands for the execution of the trajectory are calculated by each of the \textit{JointTrajectoryControllers} in the sub-controller managers individually, there is no time synchronization between the two. As a result, the execution is not started in complete synchronization and further mechanisms such as a synchronized time between the machines are necessary.

The presented work would benefit from exploring the impact of synchronized time across different computers by making use of for example the time precision protocol (PTP). In addition, Zenoh as \gls{rmw} could be investigated. As of the time of writing, there is no implementation of Zenoh as \gls{rmw} existent, but a so-called bridge for \gls{dds} exists. This should in theory allow to route all the \gls{dds} traffic through Zenoh. This however did not work and could not be further investigated. Last, the longest continuous execution of the periodic motion was around 15 minutes for scenario 1 with the Fast DDS and the sensor profile. While the robots did repeat their motion consistent, measured at industrial standards, where robots need to run for days or even weeks this is not a very long time. Therefore, a longtime test could be conducted.
% \begin{itemize}
%     \item Other middlewares
%     \item over Wi-Fi (lossy ...)
%     \item Ordered package delivery
%     \item time synchronized network (was not feasible only one network card per laptop. no special hardware and gps clock.)
%     \item longtime run (max was about 15 min=)
% \end{itemize}